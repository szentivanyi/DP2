import os
import shutil
import csv
import string
import nltk
from time import sleep as wait
import threading  # will potentially use multi-threading
from nltk.stem.porter import *
from nltk.corpus import stopwords
from nltk.corpus import wordnet
from nltk.stem.wordnet import WordNetLemmatizer
import scipy
import numpy

stopWords = set(stopwords.words('english'))


###############################################################################
########## function(s) for generating parse trees & document objects ##########
###############################################################################


def generate_document(text, author):
    """ function: generate_document
        ---------------------------
        extract class labels & tokenized (and sanitized) title/body text

        :param reuter_txt: parsetree of 'reuter' child in original parsetree
        :returns: dictionary representing fields of single document entity
    """
    document = init_document()
    populate_author(document, author)
    populate_wordlist(document, text)

    # UNCOMMENT WHEN DEBUGGING
    # print(document)
    # wait(10)

    return document


def parse_documents(type='train'):
    """ function: parse_document
        ------------------------
        extract list of Document objects from token list
        :returns: list of document entities generated by generate_document()
    """

    if 'train' in type:
        _dir_ = 'C50train'
    elif 'test' in type:
        _dir_ = 'C50test'
    else:
        raise Exception

    documents = []
    # generate well-formatted document set for each file
    for subdir in os.listdir(_dir_):
        # print(subdir)
        for file in (os.listdir(os.path.join(os.getcwd(), _dir_, subdir))):
            # print(file)
            f = open(os.path.join(os.getcwd(), _dir_, subdir, file), 'r')
            data = f.read()
            f.close()
            # print(data.lower())
            author = subdir
            data = data.lower()
            document = generate_document(data, author)
            documents.append(document)

        print(f"Finished extracting information from {_dir_}/ {subdir}")

    return documents


###############################################################################
################ function(s) for generating document objects ##################
###############################################################################


def init_document():
    """ function: init_document
        -----------------------
        initialize new empty document skeleton

        :returns: dictionary @document of document fields
        @dictionary['words'] is a dictionary
        @dictionary['body'] is a list for the body text terms
    """
    document = {
                # 'topics': [],
                'author': '',
                'body': ''
                }

    return document


def populate_class_label(document, article):
    """ function: populate_class_label
        ------------------------------
        extract topics from @article and fill @document

        :param document: formatted dictionary object representing a document
        :param article:  formatted parse tree built from unformatted data
        @article is a 'reuter' child of the original file parsetree
    """
    for topic in article.topics.children:
        document['topics'].append(topic.text.encode('ascii', 'ignore'))


def populate_author(document, author):
    """ function: populate_class_label
        ------------------------------
        extract author from @article and fill @document

        :param document: formatted dictionary object representing a document
        :param article:  formatted parse tree built from unformatted data
        @article is a 'reuter' child of the original file parsetree
    """
    if author:
        # txt = author.text #.encode('ascii', 'ignore') lebo nechcem byte-like object
        # txt = txt.split(',')[0]
        # txt = txt.strip()
        document['author'] = author


def populate_wordlist(document, text):
    """ function: populate_word_list
        ----------------------------
        extract title/body words from @article, preprocess, and fill @document

        :param document: formatted dictionary object representing a document
        :param article:  formatted parse tree built from unformatted data
            @article is a 'reuter' child of the original file parsetree
    """
    # text = article.find('text')
    # body = text.body

    if text:
        document['body'] = text


#####################################################################
################## NLTK #############################################
#####################################################################


def tokenize(text):
    """ function: tokenize
        ------------------
        generate list of tokens given a block of @text;

        :param text: string representing text field (title or body)
        :returns: list of strings of tokenized & sanitized words
    """
    # encode unicode to string
    ascii = text.encode('ascii', 'ignore')
    # remove digits
    no_digits = ascii.translate(None, string.digits)
    # remove punctuation
    no_punctuation = no_digits.translate(None, string.punctuation)
    # tokenize
    tokens = nltk.word_tokenize(no_punctuation)
    # remove stopwords - assume 'reuter'/'reuters' are also irrelevant
    no_stop_words = [w for w in tokens if not w in stopwords.words('english')]
    # filter out non-english words
    eng = [y for y in no_stop_words if wordnet.synsets(y)]
    # lemmatization process
    lemmas = []
    lmtzr = WordNetLemmatizer()
    for token in eng:
        lemmas.append(lmtzr.lemmatize(token))
    # stemming process
    stems = []
    stemmer = PorterStemmer()
    for token in lemmas:
        stems.append(stemmer.stem(token).encode('ascii','ignore'))
    # remove short stems
    terms = [x for x in stems if len(x) >= 4]

    return terms


###############################################################################

def create_db_authors(docs):
    db_authors = []
    # najdeme autorov
    for doc in docs:
        if doc['author']:
            db_authors.append(doc['author'])

    return db_authors

#######################################################################################


def process_doc(text):
    # Raw document body text
    # convert text to lower case
    text = text.lower()

    # Replace delimeter signs with whitespaces and split text to a list by whitespaces
    wordlist = text.replace(', ', ' ').replace(',\n', ' ').replace(' "', ' ').replace('" ', ' ').replace('"', ' ')\
                       .replace('. ', ' ').replace('.\n', ' ').replace('(', ' ').replace(')', ' ')\
                       .replace('>', ' ').replace('<', ' ').split()
    # DEBUG
    # print(wordlist) # list of splitted words

    # TODO '300,000'

    # Replace numbers and math signs with FLAGS
    for word in wordlist:
        if word.isdigit():
            wordlist[wordlist.index(word)] = "__cislo_int__"  # word is int number
        elif is_digit(word):
            wordlist[wordlist.index(word)] = "__cislo_float__"  # word is float number
        elif is_math_sign(word):
            wordlist[wordlist.index(word)] = "__mops__"   # word is mathematical operational sign
        else:
            # just word
            pass

    return wordlist


def is_digit(x):
    try:
        float(x)
        return True
    except ValueError:
        return False


def is_math_sign(x):
        if x == "/" or x == "*" or x == "+" or x == "-" or x == "=" or x == "%":
            return True
        else:
            return False


def remove_stopwords(wordlist):
            # stopWords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves',
            #              'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their',
            #              'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was',
            #              'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the',
            #              'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against',
            #              'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in',
            #              'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why',
            #              'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only',
            #              'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll',
            #              'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma',
            #              'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']
            #
    wordlist = [word for word in wordlist if word not in stopWords]

    return wordlist

#####################################################################################################


def create_dictionary(docs, keep_percent=80):
    # Create wordlist from docs
    wordlist = []
    for d in range(len(docs)):
        wordlist.extend(process_doc(docs[d]['body']))

    # Remove stop words
    # wordlist_no_sw = remove_stop_words(wordlist)

    # Create freq-dictionary
    wordfreq = [wordlist.count(w) for w in wordlist]
    dictionary = dict(zip(wordlist, wordfreq))
    # print(dictionary)

    # Sort freq-dictionary
    dictionary = [(dictionary[key], key) for key in dictionary]
    dictionary.sort()
    dictionary.reverse()  # from the highest to the lowest
    # print(dictionary)

    # lenght of the dictionary when we keep ?keep_percent? percent
    percent = (round(len(dictionary) / 100) * keep_percent)
    dictionary = dictionary[0:percent]
    # print(dictionary)

    print("Slovnik was created!")

    return dictionary

##############################################################################################


def word_countering(wordlist, dictionary):
        wordcounts = []

        for word in dictionary:  # v tvare list slovnikov [{12x : car}, {5x : bank}...]
            if word[-1] in wordlist:  # slovoX
                # pocet vyskytov daneho slova vo wordliste (v 1 text)
                count = wordlist.count(word[-1])
                # vyskyt daneho slova vo wordliste v percentach
                wordcounts.append((count / len(wordlist)) * 100)
            else:
                # __OOV__, slovo zo slovnika v texte nie je
                wordcounts.append(0.)

        return wordcounts


#######################################################################################
##########  feature extraction method 1  #########################
#######################################################################################


def feature_extraction_1(docs, authors):
    list_len_sentences = []
    fvs = []
    dictionary = create_dictionary(docs, keep_percent=70)

    for doc in docs:
        fv = []
        text = doc['body']

        # Mean lenght of sentences
        sentences = text.split('.')
        for sentence in sentences:
            words = sentence.split(' ')
            list_len_sentences.append(len(words))

        mean_sentence_len = scipy.mean(list_len_sentences)
        mean_sentence_len = round(mean_sentence_len, ndigits=4)
        # print("Priemerna dlzka viet: ")
        # print(mean_sentence_len)

        # Number of stopwords in document
        wordlist = process_doc(text)
        wordlist_wo_sw = remove_stopwords(wordlist)
        stopwords_count = len(wordlist) - len(wordlist_wo_sw)
        # print("Pocet stop slov: ")
        # print(stopwords_count)

        # Number of all words in document
        wordlist = process_doc(text)
        wordlist_len = len(wordlist)
        # print("Pocet all slov: ")
        # print(wordlist_len)
        # print(wordlist)

        # Standard deviation
        std = numpy.std(list_len_sentences)
        # print("Standard deviation: ")
        # print(std)

        # Word count
        wordcounts = word_countering(wordlist, dictionary)
        # print("Wordcounts: ")
        # print(wordcounts)

        # FV dokumentu
        fv.append(mean_sentence_len)
        fv.append(stopwords_count)
        fv.append(wordlist_len)
        fv.append(std)
        fv.extend(wordcounts)

        #  LV dokumentu
        lv = authors.index(doc['author'])
        fv.append(lv)
        print(f"ID authora: {lv} - {doc['author']} ")

        # FV + LV vsetkych dokumentov v liste
        fvs.append(fv)

    print("Feature extraction method 1: DONE !")


    return fvs   # all vectors (Feature Vectors + Labels) as list


#######################################################################################
##########  generate .csv with Input vector / Feature Vector  #########################
#######################################################################################

def create_cvs(dataset, authors, filename='train_data_fv.csv'):
    categories = authors
    # try:
    #     rows, features = numpy.shape(dataset)
    # except ValueError:
    #     features = len(dataset)
    #     rows = 1
    # print()
    # print(rows)
    # print(features)

    try:
        shutil.rmtree(f'/{filename}', ignore_errors=False)
    except FileNotFoundError:
        pass

    with open(filename, 'w', newline='') as f:
        writer = csv.writer(f)

       # HLAVICKA
        writer.writerow([len(dataset), len(dataset[0]) - 1, len(categories), categories])# pocet riadkov (1 riadok = 1 in/out vector = 1 dokument),
                                                                                         # pocet vlastnosti = dlzka 1 FV (-1 co je LABEL),
                                                                                         # vypisane vsetky kategorie
        # DATA
        writer.writerows(dataset)     # v riadkoch su Feature data + posledny stlpec v riadku je label

    print(f"{filename} file was created.")


###############################################################################
################## main function - single point of execution ##################
###############################################################################

def main():
    # The training corpus consists of 2,500 texts (50 per author)
    # and the test corpus includes other 2,500 texts (50 per author)
    # non-overlapping with the training texts.

    print('Generating document objects. This may take some time...')
    train = parse_documents('train')
    test = parse_documents('test')

    print('Generating list of authors. This may take some time...')
    train_authors = create_db_authors(train)
    test_authors = create_db_authors(test)


    # generate dataset 1 w lengths
    print('Generating train dataset. This may take some time...')
    trainset = feature_extraction_1(train, train_authors)

    print('Generating test dataset. This may take some time...')
    testset = feature_extraction_1(test, test_authors)

    print("dataset")
    print(len(trainset))
    print(len(trainset[0]))

    print('Generating train CSV. This may take some time...')
    create_cvs(trainset, train_authors, filename='train_data_fv1.csv')

    print('Generating test CSV. This may take some time...')
    create_cvs(testset, test_authors, filename='test_data_fv1.csv')



if __name__ == "__main__":
    main()