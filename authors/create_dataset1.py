import os
import shutil
import csv
import string
import nltk
from timeit import default_timer as timer
from time import sleep as wait
# import threading  # will potentially use multi-threading
from nltk.stem.porter import *
from nltk.corpus import stopwords
from nltk.corpus import wordnet
from nltk.stem.wordnet import WordNetLemmatizer
import scipy
from scipy.stats import moment
import numpy

#stopWords = set(stopwords.words('english'))


###############################################################################
########## function(s) for generating parse trees & document objects ##########
###############################################################################


def generate_document(text, author):

    """ function: generate_document
        ---------------------------
        extract class labels & tokenized (and sanitized) title/body text

        :param reuter_txt: parsetree of 'reuter' child in original parsetree
        :returns: dictionary representing fields of single document entity
    """
    document = init_document()
    populate_author(document, author)
    populate_wordlist(document, text)

    # UNCOMMENT WHEN DEBUGGING
    # print(document)
    # wait(10)

    return document


def parse_documents(type='train'):
    """ function: parse_document
        ------------------------
        extract list of Document objects from token list
        :returns: list of document entities generated by generate_document()
    """

    if 'train' in type:
        _dir_ = 'C50train'
    elif 'test' in type:
        _dir_ = 'C50test'
    else:
        raise Exception

    documents = []
    # generate well-formatted document set for each file
    for subdir in os.listdir(_dir_):
        # print(subdir)
        for file in (os.listdir(os.path.join(os.getcwd(), _dir_, subdir))):
            # print(file)
            f = open(os.path.join(os.getcwd(), _dir_, subdir, file), 'r')
            data = f.read()
            f.close()
            # print(data.lower())
            author = subdir
            data = data.lower()
            document = generate_document(data, author)
            documents.append(document)

        print(f"Finished extracting information from {_dir_}/ {subdir}")

    return documents


###############################################################################
################ function(s) for generating document objects ##################
###############################################################################


def init_document():
    """ function: init_document
        -----------------------
        initialize new empty document skeleton

        :returns: dictionary @document of document fields
        @dictionary['words'] is a dictionary
        @dictionary['body'] is a list for the body text terms
    """
    document = {
                # 'topics': [],
                'author': '',
                'body': ''
                }

    return document


def populate_class_label(document, article):
    """ function: populate_class_label
        ------------------------------
        extract topics from @article and fill @document

        :param document: formatted dictionary object representing a document
        :param article:  formatted parse tree built from unformatted data
        @article is a 'reuter' child of the original file parsetree
    """

    for topic in article.topics.children:
        document['topics'].append(topic.text.encode('ascii', 'ignore'))


def populate_author(document, author):
    """ function: populate_class_label
        ------------------------------
        extract author from @article and fill @document

        :param document: formatted dictionary object representing a document
        :param article:  formatted parse tree built from unformatted data
        @article is a 'reuter' child of the original file parsetree
    """
    if author:
        document['author'] = author


def populate_wordlist(document, text):
    """ function: populate_word_list
        ----------------------------
        extract title/body words from @article, preprocess, and fill @document

        :param document: formatted dictionary object representing a document
        :param article:  formatted parse tree built from unformatted data
            @article is a 'reuter' child of the original file parsetree
    """
    if text:
        document['body'] = text


#####################################################################
################## NLTK #############################################
#####################################################################


def tokenize(text):
    """ function: tokenize
        ------------------
        generate list of tokens given a block of @text;

        :param text: string representing text field (title or body)
        :returns: list of strings of tokenized & sanitized words
    """
    # encode unicode to string
    ascii = text.encode('ascii', 'ignore')
    # remove digits
    no_digits = ascii.translate(None, string.digits)
    # remove punctuation
    no_punctuation = no_digits.translate(None, string.punctuation)
    # tokenize
    tokens = nltk.word_tokenize(no_punctuation)
    # remove stopwords - assume 'reuter'/'reuters' are also irrelevant
    no_stop_words = [w for w in tokens if not w in stopwords.words('english')]
    # filter out non-english words
    eng = [y for y in no_stop_words if wordnet.synsets(y)]
    # lemmatization process
    lemmas = []
    lmtzr = WordNetLemmatizer()
    for token in eng:
        lemmas.append(lmtzr.lemmatize(token))
    # stemming process
    stems = []
    stemmer = PorterStemmer()
    for token in lemmas:
        stems.append(stemmer.stem(token).encode('ascii','ignore'))
    # remove short stems
    terms = [x for x in stems if len(x) >= 4]

    return terms


###############################################################################

def create_db_authors(docs):
    # najdeme autorov
    db_authors = [doc['author'] for doc in docs]

    # for doc in docs:
    #     if doc['author']:
    #         db_authors.append(doc['author'])

    db_authors = list(set(db_authors))
    # print('db_authors')
    # print(len(db_authors))

    return db_authors

#######################################################################################


def process_doc(text):
    # Raw document body text
    # convert text to lower case
    text = text.lower()
    # wordlist = []
    # Replace delimeter signs with whitespaces and split text to a list by whitespaces
    wordlist = text.replace(',', ' ').replace(',\n', ' ').replace(' "', ' ').replace('" ', ' ').replace('"', ' ').replace(',"', ' ')\
                       .replace('. ', ' ').replace('.\n', ' ').replace('(', ' ').replace(')', ' ')\
                       .replace('>', ' ').replace('<', ' ').replace(':', ' ').split()

    # for sentence in nltk.tokenize.sent_tokenize(text):
    #     for word in nltk.tokenize.word_tokenize(sentence):
    #         wordlist.append(word)

    # DEBUG
    # print(wordlist) # list of splitted words

    # Replace numbers and math signs with FLAGS
    for word in wordlist:
        # if word.isdigit():
        #     wordlist[wordlist.index(word)] = "__cislo_int__"  # word is int number
        if is_num(word):
            wordlist[wordlist.index(word)] = "_num_"  # word is 140,000 number
        elif is_digit(word):
            wordlist[wordlist.index(word)] = "_float_"  # word is float number
        # elif is_math_sign(word):
        #     wordlist[wordlist.index(word)] = "__mops__"   # word is mathematical operational sign
        elif is_webpage(word):
            wordlist[wordlist.index(word)] = "_web_"   # word is mathematical operational sign
        elif is_price(word):
            wordlist[wordlist.index(word)] = "_price_"   # word is mathematical operational sign
        else:
            # just word
            pass

    return wordlist


def is_num(x):
    rex = re.compile("^[0-9,]*$")
    return rex.fullmatch(x)


def is_digit(x):
    try:
       float(x)
       return True
    except ValueError:
        return False


def is_math_sign(x):
        if x == "/" or x == "*" or x == "+" or x == "-" or x == "=" or x == "%":
            return True
        else:
            return False


def is_webpage(word):
        if 'http:' in word or '.com' in word:
            return True
        else:
            return False


def is_price(word):
        if '$' in word:
            return True
        else:
            return False


def remove_stopwords(wordlist):
        stopWords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves',
                         'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their',
                         'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was',
                         'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the',
                         'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against',
                         'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in',
                         'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why',
                         'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only',
                         'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll',
                         'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma',
                         'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']

        wordlist = [word for word in wordlist if word not in stopWords]

        return wordlist

#####################################################################################################

def create_dictionary(traindocs, testdocs, keep_percent):
    docs = []
    docs.extend(traindocs)
    docs.extend(testdocs)

    # Create wordlist from docs
    wordlist = []
    for d in range(len(docs)):
        wordlist.extend(process_doc(docs[d]['body']))
    print("Slovnik 1/2 DONE!")

    # Remove stop words
    # wordlist_no_sw = remove_stop_words(wordlist)

    # for d in range(len(docs)):
    #     for sentence in nltk.tokenize.sent_tokenize(docs[d]['body']):
    #         for word in nltk.tokenize.word_tokenize(sentence):
    #             fdist[word] += 1

    # Create word frequencies
    wordcounts = nltk.FreqDist(w for w in wordlist)
    most_frequent = wordcounts.most_common(round((len(wordcounts)/100) * keep_percent))
    # print(len(most_frequent))
    # print(most_frequent)
    print("Slovnik 2/2 DONE!")

    return most_frequent


##############################################################################################

def word_frequencies(wordlist, dictionary):
    wordcounts = []
    for item in dictionary:  # v tvare list setov [('the', 30393), ('to', 14265), ...]
        if item[0] in wordlist:  # slovo
            # pocet vyskytov daneho slova vo wordliste (dokumente)
            count = wordlist.count(item[0])
            # vyskyt daneho slova vo wordliste v percentach = frekvencia
            wordcounts.append((count / len(wordlist)) * 100)
        else:
            # __OOV__, slovo zo slovnika v texte nie je
            wordcounts.append(0.)

    return wordcounts


#######################################################################################
##########  feature extraction method 1 - ALL IN ONE #########################
#######################################################################################


def feature_extraction_1(docs, authors, dictionary):
    fvs = []
    for doc in docs:
        fv = []
        text = doc['body']

        wordlist = process_doc(text)

        # Mean lenght of sentences
        # sentences = text.split('.')
        # list_len_sentences = [len(sentence.split(' ')) for sentence in sentences]
        # mean_sentence_len = scipy.mean(list_len_sentences)
        # mean_sentence_len = round(mean_sentence_len, ndigits=4)
        # print("Priemerna dlzka viet: ")
        # print(mean_sentence_len)

        # Number of stopwords in document
        # wordlist_wo_sw = remove_stopwords(wordlist)
        # stopwords_count = len(wordlist) - len(wordlist_wo_sw)
        # print("Pocet stop slov: ")
        # print(stopwords_count)

        # Number of all words in document
        # wordlist_len = len(wordlist)
        # print("Pocet all slov: ")
        # print(wordlist_len)
        # print(wordlist)

        # Standard deviation of sentence length
        # std = numpy.std(list_len_sentences)
        # print("Standard deviation: ")
        # print(std)

        # Word frequencies
        word_freqs = word_frequencies(wordlist, dictionary)
        # print("Word frequencies: ")
        # print(word_freqs)

        # Number of different words
        # vocab_richness = nltk.FreqDist(w for w in wordlist).B()
        # print(" Vocabulary richness: ")
        # print(vocab_richness)

        # Number of different words
        # num_hapaxes = len(nltk.FreqDist(w for w in wordlist).hapaxes())
        #print(" Pocet slov, ktore pouzil autor len raz: ")
        #print(num_hapaxes)

        # MOMENTS
        # m1 = moment(a=list_len_sentences, moment=1)  # always 0
        # m2 = moment(a=list_len_sentences, moment=2)  # variance je 2 mocnina std
        # m3 = moment(a=list_len_sentences, moment=3)  # skewness - sikmost
        # m4 = moment(a=list_len_sentences, moment=4)  # kurtosis - spicatost
        # m5 = moment(a=list_len_sentences, moment=5)  # -
        # m6 = moment(a=list_len_sentences, moment=6)  # -
        # m7 = moment(a=list_len_sentences, moment=7)  # -
        # m8 = moment(a=list_len_sentences, moment=8)  # -
        # m9 = moment(a=list_len_sentences, moment=9)  # -
        # m10 = moment(a=list_len_sentences, moment=10)  # -

        # FV dokumentu
        # fv.append(mean_sentence_len)
        # fv.append(stopwords_count)
        # fv.append(wordlist_len)
        # fv.append(std)
        # fv.append(vocab_richness)
        # fv.append(num_hapaxes)
        # fv.append(m1)
        # fv.append(m2)
        # fv.append(m3)
        # fv.append(m4)
        # fv.append(m5)
        # fv.append(m6)
        # fv.append(m7)
        # fv.append(m8)
        # fv.append(m10)

        fv.extend(word_freqs)

        #  LV dokumentu
        lv = authors.index(doc['author'])
        fv.append(lv)

        # FV + LV vsetkych dokumentov v liste
        fvs.append(fv)

    print("Feature extraction method 1: DONE !")

    return fvs   # all vectors (Feature Vectors + Labels) as list

#######################################################################################
##########  feature extraction method 2  #########################
#######################################################################################


def feature_extraction_2(docs, authors):
    fvs = []
    for doc in docs:
        fv = []
        text = doc['body']

        # Mean lenght of sentences
        sentences = text.split('.')
        list_len_sentences = [len(sentence.split(' ')) for sentence in sentences]
        mean_sentence_len = scipy.mean(list_len_sentences)
        mean_sentence_len = round(mean_sentence_len, ndigits=6)
        # print("Priemerna dlzka viet: ")
        # print(mean_sentence_len)

        # Number of stopwords in document
        wordlist = process_doc(text)
        wordlist_wo_sw = remove_stopwords(wordlist)
        stopwords_count = len(wordlist) - len(wordlist_wo_sw)
        # print("Pocet stop slov: ")
        # print(stopwords_count)

        # Number of all words in document
        wordlist = process_doc(text)
        wordlist_len = len(wordlist)
        # print("Pocet all slov: ")
        # print(wordlist_len)
        # print(wordlist)

        # Standard deviation
        std = numpy.std(list_len_sentences)
        # print("Standard deviation: ")
        # print(std)

        # FV dokumentu
        fv.append(mean_sentence_len)
        fv.append(stopwords_count)
        fv.append(wordlist_len)
        fv.append(std)

        #  LV dokumentu
        lv = authors.index(doc['author'])
        fv.append(lv)

        # FV + LV vsetkych dokumentov v liste
        fvs.append(fv)

    print("Feature extraction method 2: DONE !")

    return fvs   # all vectors (Feature Vectors + Labels) as list

###################################################################################
##########  feature extraction method 3  #########################
###################################################################################

def feature_extraction_3(docs, authors):
    fvs = []
    for doc in docs:
        fv = []
        text = doc['body']

        # Mean lenght of sentences
        sentences = text.split('.')
        list_len_sentences = [len(sentence.split(' ')) for sentence in sentences]

        # mean_sentence_len = scipy.mean(list_len_sentences)
        # mean_sentence_len = round(mean_sentence_len, ndigits=4)
        # print("\nPriemerna dlzka viet: ")
        # print(mean_sentence_len)

        # m0 = moment(a=list_len_sentences, moment=0)  # always 1
        m1 = moment(a=list_len_sentences, moment=1)  # always 0
        m2 = moment(a=list_len_sentences, moment=2)  # variance je 2 mocnina std
        m3 = moment(a=list_len_sentences, moment=3)  # skewness - sikmost
        m4 = moment(a=list_len_sentences, moment=4)  # kurtosis - spicatost
        m5 = moment(a=list_len_sentences, moment=5)  #  -
        m6 = moment(a=list_len_sentences, moment=6)  #  -
        m7 = moment(a=list_len_sentences, moment=7)  #  -
        m8 = moment(a=list_len_sentences, moment=8)  #  -
        m9 = moment(a=list_len_sentences, moment=9)  #  -
        m10 = moment(a=list_len_sentences, moment=10)  #  -

        # print("MOMENT: ")
        # print(m0)
        # print(m1)
        # print(m2)
        # print(m3)
        # print(m4)
        # print('varciancia ala 2 moment')
        # var = numpy.var(list_len_sentences)
        # print(var)

        # Number of stopwords in document
        # wordlist = process_doc(text)
        # wordlist_wo_sw = remove_stopwords(wordlist)
        # stopwords_count = len(wordlist) - len(wordlist_wo_sw)
        # print("Pocet stop slov: ")
        # print(stopwords_count)

        # Number of all words in document
        # wordlist = process_doc(text)
        # wordlist_len = len(wordlist)
        # print("Pocet all slov: ")
        # print(wordlist_len)
        # print(wordlist)

        # Standard deviation
        # std = numpy.std(list_len_sentences)
        # print("Standard deviation: ")
        # print(std)

        # FV dokumentu
        fv.append(m1)
        fv.append(m2)
        fv.append(m3)
        fv.append(m4)
        fv.append(m5)
        fv.append(m6)
        fv.append(m7)
        fv.append(m8)
        fv.append(m10)
        # fv.append(stopwords_count)
        # fv.append(wordlist_len)
        # fv.append(std)

        #  LV dokumentu
        lv = authors.index(doc['author'])
        fv.append(lv)

        # FV + LV vsetkych dokumentov v liste
        fvs.append(fv)

    print("Feature extraction method 3: DONE !")

    return fvs   # all vectors (Feature Vectors + Labels) as list


#######################################################################################
##########  feature extraction method 4  #########################
#######################################################################################

def feature_extraction_4(docs, authors):
    fvs = []
    for doc in docs:
        fv = []
        text = doc['body']

        # Mean lenght of sentences
        sentences = text.split('.')
        list_len_sentences = [len(sentence.split(' ')) for sentence in sentences]

        # mean_sentence_len = scipy.mean(list_len_sentences)
        # mean_sentence_len = round(mean_sentence_len, ndigits=4)
        # print("\nPriemerna dlzka viet: ")
        # print(mean_sentence_len)

        # m0 = moment(a=list_len_sentences, moment=0)  # always 1
        m1 = moment(a=list_len_sentences, moment=1)  # always 0
        m2 = moment(a=list_len_sentences, moment=2)  # variance je 2 mocnina std
        m3 = moment(a=list_len_sentences, moment=3)  # skewness - sikmost
        m4 = moment(a=list_len_sentences, moment=4)  # kurtosis - spicatost
        m5 = moment(a=list_len_sentences, moment=5)  #  -
        m6 = moment(a=list_len_sentences, moment=6)  #  -
        m7 = moment(a=list_len_sentences, moment=7)  #  -

        # Number of stopwords in document
        # wordlist = process_doc(text)
        # wordlist_wo_sw = remove_stopwords(wordlist)
        # stopwords_count = len(wordlist) - len(wordlist_wo_sw)
        # print("Pocet stop slov: ")
        # print(stopwords_count)

        # Number of all words in document
        # wordlist = process_doc(text)
        # wordlist_len = len(wordlist)
        # print("Pocet all slov: ")
        # print(wordlist_len)
        # print(wordlist)

        # Standard deviation
        # std = numpy.std(list_len_sentences)
        # print("Standard deviation: ")
        # print(std)

        # FV dokumentu
        fv.append(m1)
        fv.append(m2)
        fv.append(m3)
        fv.append(m4)
        fv.append(m5)
        fv.append(m6)
        fv.append(m7)
        # fv.append(stopwords_count)
        # fv.append(wordlist_len)
        # fv.append(std)

        #  LV dokumentu
        lv = authors.index(doc['author'])
        fv.append(lv)

        # FV + LV vsetkych dokumentov v liste
        fvs.append(fv)

    print("Feature extraction method 4: DONE !")

    return fvs   # all vectors (Feature Vectors + Labels) as list



#######################################################################################
##########  generate .csv with Input vector / Feature Vector  #########################
#######################################################################################

def create_cvs(dataset, authors, filename='train_data_fv.csv'):
    categories = authors

    try:
        shutil.rmtree(f'/{filename}', ignore_errors=False)
    except FileNotFoundError:
        pass

    with open(filename, 'w', newline='') as f:
        writer = csv.writer(f)

       # HLAVICKA
        writer.writerow([len(dataset), len(dataset[0]) - 1, len(categories), categories])# pocet riadkov (1 riadok = 1 in/out vector = 1 dokument),
                                                                                         # pocet vlastnosti = dlzka 1 FV (-1 co je LABEL),
                                                                                         # vypisane vsetky kategorie
        # DATA
        writer.writerows(dataset)     # v riadkoch su Feature data + posledny stlpec v riadku je label

    print(f"{filename} file was created.\n")


###############################################################################
################## main function - single point of execution ##################
###############################################################################

def main():
    # The training corpus consists of 2,500 texts (50 per author)
    # and the test corpus includes other 2,500 texts (50 per author)
    # non-overlapping with the training texts.

    print('\n Generating document objects...')
    train = parse_documents('train')
    test = parse_documents('test')

    print('\n Generating list of authors...')
    train_authors = create_db_authors(train)
    test_authors = create_db_authors(test)

    assert len(train_authors) == len(test_authors), "PROBLEM: pocet test a train autorov sa musi rovnat."

    print("\n Generating dictionary...")
    dict = create_dictionary(train[:500], test[:500], keep_percent=80)

    ####################################
    ###       GENERATE DATASET       ###
    ####################################

    filename_train = 'train_data_fv15.csv'  # MUST BE CHANGED !
    filename_test = 'test_data_fv15.csv'  # MUST BE CHANGED !

    ### TRAIN FV ###
    print('\nGenerating train dataset. This may take some time...')
    trainset = feature_extraction_1(train[:500], train_authors, dict)    # priemerna dlzka viet + pocet stopslov + pocet vsetkych slov + standard deviation + frekvencie slov + 10 MOMENTS + hapaxes + richness +
    # trainset = feature_extraction_2(train, train_authors)        # priemerna dlzka viet + pocet stopslov + pocet vsetkych slov + standard deviation
    # trainset = feature_extraction_3(train, train_authors)        # prvych 10 MOMENTOV
    # trainset = feature_extraction_4(train, train_authors)


    ### TEST FV ###
    print('\nGenerating test dataset. This may take some time...')
    testset = feature_extraction_1(test[:500], test_authors, dict)
    # testset = feature_extraction_2(test, test_authors)
    # testset = feature_extraction_3(test, test_authors)
    # testset = feature_extraction_4(test, test_authors)


    assert len(testset[0]) == len(trainset[0]), "PROBLÉM: dĺžka testovacieho a trénovacieho FV sa nerovná."


    print('Generating train CSV. This may take some time...')
    create_cvs(trainset, train_authors, filename=filename_train)

    print('Generating test CSV. This may take some time...')
    create_cvs(testset, test_authors, filename=filename_test)


if __name__ == "__main__":
    main()